---
- name: Deploy Ollama and Open WebUI with GPU support and SRAM authentication
  hosts: localhost
  become: yes
  gather_facts: true

  vars:
    app_dir: /opt/local-llm
    ollama_models:
      - mistral:7b
      - gpt-oss:20b
                
    
    # RSC Catalog variables (these will be provided by SURF Research Cloud)
    # public_fqdn: "{{ rsc_nginx_service_url }}"  # Will be set by RSC
    # The nginx plugin will handle SSL/TLS and basic auth setup

  pre_tasks:
    - name: Wait for system to become reachable
      wait_for_connection:
        timeout: 300

    - name: Gather facts
      setup:

    - name: Update apt cache (Debian/Ubuntu)
      apt:
        update_cache: yes
        cache_valid_time: 3600
      when: ansible_os_family == "Debian"

  tasks:
    # ============================================================
    # GPU Prerequisites Check
    # ============================================================
    - name: Check for NVIDIA GPU
      shell: lspci | grep -i nvidia
      register: nvidia_check
      failed_when: false
      changed_when: false

    - name: Fail if no NVIDIA GPU detected
      fail:
        msg: "No NVIDIA GPU detected. This playbook requires an NVIDIA GPU."
      when: nvidia_check.rc != 0

    - name: Check if NVIDIA drivers are installed
      shell: nvidia-smi
      register: nvidia_smi_check
      failed_when: false
      changed_when: false

    - name: Display NVIDIA driver status
      debug:
        msg: "NVIDIA drivers {{ 'are installed' if nvidia_smi_check.rc == 0 else 'are NOT installed' }}"

    - name: Warning if NVIDIA drivers not found
      debug:
        msg: 
          - "WARNING: nvidia-smi not found. NVIDIA drivers may not be installed."
          - "Please install NVIDIA drivers manually if needed."
          - "For Ubuntu: sudo ubuntu-drivers autoinstall"
          - "Continuing with playbook assuming drivers will be available..."
      when: nvidia_smi_check.rc != 0

    # ============================================================
    # Docker Installation
    # ============================================================
    - name: Install Docker prerequisites (Debian/Ubuntu)
      apt:
        name: 
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
          - python3-pip
        state: present
      when: ansible_os_family == "Debian"

    - name: Add Docker GPG key (Debian/Ubuntu)
      shell: |
        install -m 0755 -d /etc/apt/keyrings
        curl -fsSL https://download.docker.com/linux/{{ ansible_facts['distribution'] | lower }}/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
        chmod a+r /etc/apt/keyrings/docker.gpg
      args:
        creates: /etc/apt/keyrings/docker.gpg
      when: ansible_os_family == "Debian"

    - name: Add Docker repository (Debian/Ubuntu)
      copy:
        dest: /etc/apt/sources.list.d/docker.list
        content: |
          deb [arch={{ 'amd64' if ansible_architecture == 'x86_64' else ansible_architecture }} signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/{{ ansible_facts['distribution'] | lower }} {{ ansible_facts['distribution_release'] }} stable
      when: ansible_os_family == "Debian"

    - name: Install Docker Engine and Compose plugin
      apt:
        update_cache: yes
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
          - docker-buildx-plugin
          - docker-compose-plugin
        state: present
      when: ansible_os_family == "Debian"

    - name: Install Docker (RedHat/CentOS)
      yum:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
          - docker-buildx-plugin
          - docker-compose-plugin
        state: present
      when: ansible_os_family == "RedHat"

    - name: Ensure Docker service is running and enabled
      service:
        name: docker
        state: started
        enabled: yes

    # ============================================================
    # NVIDIA Container Toolkit Installation
    # ============================================================
    - name: Check if NVIDIA Container Toolkit is installed
      shell: dpkg -l | grep nvidia-container-toolkit
      register: nvidia_toolkit_check
      failed_when: false
      changed_when: false
      when: ansible_os_family == "Debian"

    - name: Add NVIDIA Container Toolkit GPG key (Debian/Ubuntu)
      shell: |
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      args:
        creates: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      when: ansible_os_family == "Debian" and nvidia_toolkit_check.rc != 0

    - name: Add NVIDIA Container Toolkit repository (Debian/Ubuntu)
      shell: |
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
          sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
          tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
      when: ansible_os_family == "Debian" and nvidia_toolkit_check.rc != 0

    - name: Update apt cache after adding NVIDIA repository
      apt:
        update_cache: yes
      when: ansible_os_family == "Debian" and nvidia_toolkit_check.rc != 0

    - name: Install NVIDIA Container Toolkit (Debian/Ubuntu)
      apt:
        name: nvidia-container-toolkit
        state: present
      when: ansible_os_family == "Debian"

    - name: Configure Docker to use NVIDIA runtime
      shell: |
        nvidia-ctk runtime configure --runtime=docker
      notify: restart docker

    - name: Flush handlers to restart Docker if needed
      meta: flush_handlers

    # ============================================================
    # Verify GPU Access in Docker
    # ============================================================
    - name: Test GPU access in Docker
      shell: docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
      register: gpu_test
      failed_when: false
      changed_when: false

    - name: Display GPU test results
      debug:
        var: gpu_test.stdout_lines
      when: gpu_test.rc == 0

    - name: Fail if GPU not accessible in Docker
      fail:
        msg: "GPU is not accessible in Docker containers. Please check NVIDIA drivers and Container Toolkit installation."
      when: gpu_test.rc != 0

    # ============================================================
    # Application Directory Setup
    # ============================================================
    - name: Create application directory
      file:
        path: "{{ app_dir }}"
        state: directory
        mode: "0755"

    # ============================================================
    # Docker Compose Configuration with GPU Support
    # ============================================================
    - name: Write docker-compose.yml for Ollama and Open WebUI with GPU
      copy:
        dest: "{{ app_dir }}/docker-compose.yml"
        mode: "0644"
        content: |
          version: "3.9"
          
          services:
            ollama:
              image: ollama/ollama:latest
              container_name: ollama
              restart: unless-stopped
              ports:
                - "127.0.0.1:11434:11434"
              volumes:
                - ollama-data:/root/.ollama
              deploy:
                resources:
                  reservations:
                    devices:
                      - driver: nvidia
                        count: all
                        capabilities: [gpu]
              healthcheck:
                test: ["CMD", "ollama", "list"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s

            open-webui:
              image: ghcr.io/open-webui/open-webui:main
              container_name: open-webui
              depends_on:
                ollama:
                  condition: service_healthy
              environment:
                - OLLAMA_BASE_URL=http://ollama:11434
                - WEBUI_AUTH=False
                - WEBUI_NAME=SURF Local LLM (GPU)
                - DEFAULT_USER_ROLE=user
                - ENABLE_SIGNUP=False
              volumes:
                - open-webui-data:/app/backend/data
              ports:
                - "127.0.0.1:8080:8080"
              restart: unless-stopped

          volumes:
            ollama-data:
            open-webui-data:

    # ============================================================
    # Deploy Docker Compose Stack
    # ============================================================
    - name: Start Ollama and Open WebUI services with GPU
      shell: |
        cd {{ app_dir }}
        docker compose up -d
      args:
        chdir: "{{ app_dir }}"

    - name: Wait for Ollama to be healthy
      shell: docker inspect -f {% raw %}'{{.State.Health.Status}}'{% endraw %} ollama
      register: ollama_health
      until: ollama_health.stdout == 'healthy'
      retries: 30
      delay: 10

    - name: Verify GPU is available in Ollama container
      shell: docker exec ollama nvidia-smi
      register: ollama_gpu_check
      failed_when: false
      changed_when: false

    - name: Display Ollama GPU status
      debug:
        var: ollama_gpu_check.stdout_lines
      when: ollama_gpu_check.rc == 0

    - name: Pull Ollama models
      shell: docker exec ollama ollama pull {{ item }}
      loop: "{{ ollama_models }}"
      register: model_pull_result
      retries: 3
      delay: 5
      until: model_pull_result.rc == 0

    - name: Verify models are available
      shell: docker exec ollama ollama list | grep -q "{{ item.split(':')[0] }}"
      loop: "{{ ollama_models }}"
      register: model_check
      retries: 5
      delay: 5
      until: model_check.rc == 0

    # ============================================================
    # Nginx Configuration for SRAM Authentication
    # ============================================================
    - name: Create nginx app location config for Open WebUI
      copy:
        dest: /etc/nginx/app-location-conf.d/openwebui.conf
        mode: 0644
        content: |
          # Open WebUI Main Application
          location / {
              error_page 401 = @custom_401;
              auth_request /validate;
              auth_request_set $username $upstream_http_username;
              
              proxy_pass http://127.0.0.1:8080;
              proxy_http_version 1.1;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection $connection_upgrade;
              proxy_set_header REMOTE_USER $username;
              
              # Extended timeouts for LLM streaming
              proxy_connect_timeout 600;
              proxy_send_timeout 600;
              proxy_read_timeout 600;
              
              # Disable buffering for streaming responses
              proxy_buffering off;
              proxy_request_buffering off;
              chunked_transfer_encoding on;
          }

          # WebSocket support for real-time updates
          location /ws {
              error_page 401 = @custom_401;
              auth_request /validate;
              
              proxy_pass http://127.0.0.1:8080/ws;
              proxy_http_version 1.1;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection $connection_upgrade;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              
              proxy_read_timeout 86400;
          }

          # API endpoints for Open WebUI
          location /api {
              error_page 401 = @custom_401;
              auth_request /validate;
              auth_request_set $username $upstream_http_username;
              
              proxy_pass http://127.0.0.1:8080/api;
              proxy_http_version 1.1;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_set_header REMOTE_USER $username;
              
              proxy_connect_timeout 600;
              proxy_send_timeout 600;
              proxy_read_timeout 600;
              proxy_buffering off;
              proxy_request_buffering off;
              chunked_transfer_encoding on;
          }

          # Ollama API endpoints (if needed for direct access)
          location /ollama/ {
              error_page 401 = @custom_401;
              auth_request /validate;
              
              rewrite ^/ollama/(.*)$ /$1 break;
              proxy_pass http://127.0.0.1:11434;
              proxy_http_version 1.1;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              
              proxy_buffering off;
              chunked_transfer_encoding on;
          }
      notify: reload nginx

    # ============================================================
    # Nginx Map for WebSocket Upgrade
    # ============================================================
    - name: Ensure map directive for WebSocket upgrade exists
      blockinfile:
        path: /etc/nginx/conf.d/map-upgrade.conf
        create: yes
        mode: 0644
        marker: "# {mark} ANSIBLE MANAGED BLOCK - WebSocket"
        block: |
          map $http_upgrade $connection_upgrade {
              default upgrade;
              ''      close;
          }
      notify: reload nginx

  handlers:
    - name: restart docker
      service:
        name: docker
        state: restarted

    - name: reload nginx
      service:
        name: nginx
        state: reloaded

  post_tasks:
    - name: Display deployment information
      debug:
        msg:
          - "=================================="
          - "Ollama + Open WebUI GPU Deployment Complete"
          - "=================================="
          - "Service URL: https://{{ rsc_nginx_service_url | default('your-domain.nl') }}"
          - "Ollama Models: {{ ollama_models | join(', ') }}"
          - "GPU Acceleration: ENABLED"
          - ""
          - "Authentication: SRAM SSO"
          - "Users must log in with their institutional credentials"
          - ""
          - "Services running:"
          - "  - Ollama (GPU): http://localhost:11434"
          - "  - Open WebUI: http://localhost:8080"
          - "  - Public access via nginx with SRAM auth"
          - ""
          - "GPU Status: Check with 'docker exec ollama nvidia-smi'"
          - "=================================="


